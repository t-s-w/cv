rownames(data) <- NULL
termlist <- data$term
termlist <- gsub('[^a-z]','',termlist)
termlist <- unique(termlist)
termlist <- termlist[-match('',termlist)]
termlist <- termlist[!grepl(profanityregex,termlist)]
saveRDS(termlist,'termlist.RDS')
tdm <- TermDocumentMatrix(CORPUS)
terms <- rownames(as.matrix(tdm))
freqs <- rowSums(as.matrix(tdm))
data <- data.frame(term = terms, Frequency = freqs)
data <- data[order(data$Frequency,decreasing = TRUE),]
data <- data[data$Frequency > 1,]
rownames(data) <- NULL
termlist <- data$term
termlist <- gsub('[^a-z]','',termlist)
termlist <- unique(termlist)
termlist <- termlist[-match('',termlist)]
termlist <- termlist[!grepl(profanityregex,termlist)]
saveRDS(termlist,'termlist.RDS')
rownames(data) <- NULL
termlist <- data$term
termlist <- gsub('[^a-z]','',termlist)
termlist <- unique(termlist)
termlist <- termlist[-match('',termlist)]
termlist <- data$term
termlist <- gsub('[^a-z]','',termlist)
termlist <- unique(termlist)
match('',termlist)
sum(termlist == '')
termlist <- termlist[!grepl(profanityregex,termlist)]
saveRDS(termlist,'termlist.RDS')
runApp('GitHub/projects/text-predict')
stringclean <- function(x) {
x <- gsub("http:\\S+","",x)  #URLs
x <- gsub("\\swww.\\S+","",x)#URLs not including http
x <- gsub("#\\S+","",x)      #hashtags
x <- gsub("@\\S+","",x)      #twitter mentions
x <- gsub('[^a-zA-Z ]','',x)
x <- tolower(x)
x
}
tokenize <- function(x) {
wordvector <- strsplit(stringclean(x)," ")[[1]]
n <- length(wordvector)
if(n > 4) {
last4words <- wordvector[(n-3):n]
} else {
last4words <- wordvector
}
n <- length(last4words)
check <- last4words[-n] %in% termlist
check2 <- which(check %in% FALSE)
if(length(check2) > 0) {
i <- max(check2)
last4words <- last4words[(i+1):4]
}
last4words
}
wordsearch2 <- function(string,n,result,data){
ngram <- tokenize(string)
k <- length(ngram)
if(k < n-1) {
return(result)
}
wordvector <- strsplit(stringclean(string),' ')[[1]]
i <- length(wordvector)
if(i > n){
stringminus <- paste(wordvector[1:(i-n)],collapse = ' ')
} else {
stringminus <- NULL
}
if(i+1 > n){
stringplus <- paste(wordvector[1:(i-n+1)],collapse = ' ')
} else {
stringplus <- NULL
}
subngram <- rev(rev(ngram)[1:n])
subngram2 <- rev(rev(ngram)[1:(n-1)])
if(!grepl(' $',string) & n <= k){
if(length(result) < 5) {
query <- paste(subngram,collapse = ' ')
search <- grep(paste0('^',query,'[a-z]'),data$term)
if(length(search) > 0) {
top5 <- as.character(data$term[search])
if(length(top5) > 5) {
top5 <- top5[1:5]
}
if(!is.null(stringminus)) {
top5 <- paste(stringminus,top5)
}
result <- c(result, top5)
}
}
}
if(length(result) < 5 & n <= k+1) {
query <- paste(subngram2,collapse = ' ')
search <- grep(paste0('^',query,' '),data$term)
if(length(search) > 0) {
top5 <- as.character(data$term[search])
if(length(top5) > 5) {
top5 <- top5[1:5]
}
if(!is.null(stringplus)){
top5 <- paste(stringplus,top5)
}
result <- c(result,top5)
}
}
result
}
wordsearch <- function(string,n,result) {
if(n == 2) {
result <- wordsearch2(string,n,result,bigramdata)
} else if(n == 3) {
result <- wordsearch2(string,n,result,trigramdata)
} else {
result <- wordsearch2(string,n,result,tetragramdata)
}
if(length(result)>5) {
result <- result[1:5]
}
result
}
'wordsearch2('hello fucker how',2,bigramdata)
'wordsearch2('hello fucker how',2,bigramdata)
wordsearch2('hello fucker how',2,bigramdata)
wordsearch('hello fucker how',2,bigramdata)
wordsearch('hello fucker how',2,character())
wordsearch2('hello fucker how',2,character(),bigramdata)
ngram <- tokenize(string)
k <- length(ngram)
if(k < n-1) {
return(result)
}
wordvector <- strsplit(stringclean(string),' ')[[1]]
i <- length(wordvector)
if(i > n){
stringminus <- paste(wordvector[1:(i-n)],collapse = ' ')
} else {
stringminus <- NULL
}
if(i+1 > n){
stringplus <- paste(wordvector[1:(i-n+1)],collapse = ' ')
} else {
stringplus <- NULL
}
string <- 'hello fucker how'
n <- 2
result <- character()
data <- bigramdata
ngram <- tokenize(string)
k <- length(ngram)
if(k < n-1) {
return(result)
}
wordvector <- strsplit(stringclean(string),' ')[[1]]
i <- length(wordvector)
if(i > n){
stringminus <- paste(wordvector[1:(i-n)],collapse = ' ')
} else {
stringminus <- NULL
}
if(i+1 > n){
stringplus <- paste(wordvector[1:(i-n+1)],collapse = ' ')
} else {
stringplus <- NULL
}
ngram
wordvector <- strsplit(stringclean(x)," ")[[1]]
n <- length(wordvector)
if(n > 4) {
last4words <- wordvector[(n-3):n]
} else {
last4words <- wordvector
}
n <- length(last4words)
check <- last4words[-n] %in% termlist
check2 <- which(check %in% FALSE)
check
hello %in% termlist
'hello' %in% termlist
last4words
x <- 'hello fucker how'
tokenize(x)
tokenize <- function(x) {
wordvector <- strsplit(stringclean(x)," ")[[1]]
n <- length(wordvector)
if(n > 4) {
last4words <- wordvector[(n-3):n]
} else {
last4words <- wordvector
}
n <- length(last4words)
check <- last4words[-n] %in% termlist
check2 <- which(check %in% FALSE)
if(length(check2) > 0) {
i <- max(check2)
last4words <- last4words[(i+1):4]
}
last4words
}
tokenize(x)
x
wordvector <- strsplit(stringclean(x)," ")[[1]]
n <- length(wordvector)
if(n > 4) {
last4words <- wordvector[(n-3):n]
} else {
last4words <- wordvector
}
wordvector
last4wrods
last4words
n <- length(last4words)
check <- last4words[-n] %in% termlist
check
tokenize <- function(x) {
wordvector <- strsplit(stringclean(x)," ")[[1]]
n <- length(wordvector)
if(n > 4) {
last4words <- wordvector[(n-3):n]
} else {
last4words <- wordvector
}
n <- length(last4words)
check <- last4words[-n] %in% termlist
check2 <- which(check %in% FALSE)
if(length(check2) > 0) {
i <- max(check2)
last4words <- last4words[(i+1):n]
}
last4words
}
tokenize('hello fucker how')
runApp('GitHub/projects/text-predict')
tokenize('a lot of eo')
tokenize('a lot of peo')
'of' %in% termlist
termlist
terms <- rownames(as.matrix(tdm))
freqs <- rowSums(as.matrix(tdm))
data <- data.frame(term = terms, Frequency = freqs)
data <- data[order(data$Frequency,decreasing = TRUE),]
data <- data[data$Frequency > 1,]
rownames(data) <- NULL
termlist <- data$term
'of' %in% data%term
'of' %in% data$term
'a' %in% data$term
'in' %in% data$term
'hi' %in% data$term
'hello' %in% data$term
'u' %in% data$term
'hi' %in% data$term
'mm' %in% data$term
'u' %in% data$term
'vagina' %in% data$term
'of' %in% data$term
words
words()
head(trigramdata$term)
head(tetragramdata$term)
head(vigramdata$term)
head(bigramdata$term)
tdm <- TermDocumentMatrix(CORPUS,control=list(wordLengths=c(1,Inf)))
terms <- rownames(as.matrix(tdm))
freqs <- rowSums(as.matrix(tdm))
data <- data.frame(term = terms, Frequency = freqs)
data <- data[order(data$Frequency,decreasing = TRUE),]
data <- data[data$Frequency > 1,]
rownames(data) <- NULL
termlist <- data$term
termlist <- gsub('[^a-z]','',termlist)
termlist <- unique(termlist)
termlist <- termlist[!grepl(profanityregex,termlist)]
saveRDS(termlist,'termlist.RDS')
'of' %in% termlist
runApp('GitHub/projects/text-predict')
runApp('GitHub/projects/text-predict')
runApp('GitHub/projects/text-predict')
runApp('GitHub/projects/text-predict')
?trimws
runApp('GitHub/projects/text-predict')
runApp('GitHub/projects/text-predict')
runApp('GitHub/projects/text-predict')
trigramdata$term[grepl('^must be ',trigramdata$term)]
trigramdata$term[grepl('^must be j',trigramdata$term)]
trigramdata$term[grepl('^must be j',trigramdata$term)]
trigramdata$term[grepl('^must be k',trigramdata$term)]
head(news)
sum(profanity %in% news)
sum(profanity %in% blogs)
sum(profanity %in% twitter)
profanity
profanenews <- logical(length(news))
profaneblogs <- logical(length(blogs))
profanetwitter <- logical(length(twitter))
for(i in length(profanenews)) {profanenews[i] <- sum(profanity %in% news[i]) == 0}
for(i in length(profaneblogs)) {profaneblogs[i] <- sum(profanity %in% blogs[i]) == 0}
for(i in length(profanetwitter)) {profanetwitter[i] <- sum(profanity %in% twitter[i]) == 0}
sum(profanenews)
sum(profaneblogs)
sum(profanetwitter)
profanenews
runApp('GitHub/projects/text-predict')
runApp('GitHub/projects/text-predict')
?sample
cleaning <- function(x) {
x <- gsub("http:\\S+","",x)  #URLs
x <- gsub("\\swww.\\S+","",x)#URLs not including http
x <- gsub("#\\S+","",x)      #hashtags
x <- gsub("@\\S+","",x)      #twitter mentions
x <- gsub('[^a-zA-Z ]','',x)
x
}
alltext <- c(news,blogs,twitter)
set.seed(5318008)
textsample <- sample(alltext,10000) %>%
cleaning()
head(textsample)
testsample <- gsub(' \\S+$','',textsample)
head(testsample)
stringclean <- function(x) {
x <- gsub("http:\\S+","",x)  #URLs
x <- gsub("\\swww.\\S+","",x)#URLs not including http
x <- gsub("#\\S+","",x)      #hashtags
x <- gsub("@\\S+","",x)      #twitter mentions
x <- gsub('[^a-zA-Z ]','',x)
x <- tolower(x)
x
}
tokenize <- function(x) {
wordvector <- strsplit(stringclean(x)," ")[[1]]
n <- length(wordvector)
if(n > 4) {
last4words <- wordvector[(n-3):n]
} else {
last4words <- wordvector
}
n <- length(last4words)
check <- last4words[-n] %in% termlist
check2 <- which(check %in% FALSE)
if(length(check2) > 0) {
i <- max(check2)
last4words <- last4words[(i+1):n]
}
last4words
}
wordsearch2 <- function(string,n,result,data){
ngram <- tokenize(string)
k <- length(ngram)
if(k < n-1) {
return(result)
}
subngram <- rev(rev(ngram)[1:n])
subngram2 <- rev(rev(ngram)[1:(n-1)])
if(!grepl(' $',string) & n <= k){
if(length(result) < 5) {
query <- paste(subngram,collapse = ' ')
search <- grep(paste0('^',query,'[a-z]'),data$term)
if(length(search) > 0) {
top5 <- as.character(data$term[search])
if(length(top5) > 5) {
top5 <- top5[1:5]
}
top5 <- gsub(paste0('^',query),'',top5)
top5 <- paste0(string,top5)
result <- c(result, top5)
}
}
}
if(length(result) < 5 & n <= k+1) {
query <- paste(subngram2,collapse = ' ')
search <- grep(paste0('^',query,' '),data$term)
if(length(search) > 0) {
top5 <- as.character(data$term[search])
if(length(top5) > 5) {
top5 <- top5[1:5]
}
top5 <- gsub(paste0('^',query),'',top5)
top5 <- paste0(trimws(string),top5)
result <- c(result,top5)
}
}
result
}
wordsearch <- function(string) {
result <- wordsearch2(string,4,character(),tetragramdata)
result <- wordsearch2(string,3,result,trigramdata)
result <- wordsearch2(string,2,result,bigramdata)
if(length(result)>5) {
result <- result[1:5]
}
result
}
testsample <- gsub(' \\S+$','',textsample)
result <- logical(length(testsample))
for(i in 1:length(testsample)) {
result[i] <- sum(wordsearch(testsample[i]) %in% textsample[i])
}
testsample <- gsub(' \\S+$','',textsample)
result <- logical(length(testsample))
for(i in 1:length(testsample)) {
result[i] <- sum(wordsearch(testsample[i]) %in% textsample[i]) > 0
}
sum(result)
sum(result) / length(result)
length(news)
length(blogs)
length(twitter)
# Chunk 1
plot(cars)
?withprogress
??withprogress
gram4 <- TermDocumentMatrix(CORPUS, control = list(tokenize = tetragram))
tetragramterms <- rownames(as.matrix(gram4))
tetragramfreqs <- rowSums(as.matrix(gram4))
tetragramdata <- data.frame(term = tetragramterms, Frequency = tetragramfreqs)
tetragramdata <- tetragramdata[order(tetragramdata$Frequency,decreasing=TRUE),]
tetragramdata <- tetragramdata[tetragramdata$Frequency > 2,]
tetragramdata <- tetragramdata[!grepl(profanityregex,tetragramdata$term),]
rownames(tetragramdata) <- NULL
require(stringr)
library(tm)
library(dplyr)
library(ggplot2)
gram4 <- TermDocumentMatrix(CORPUS, control = list(tokenize = tetragram))
tetragramterms <- rownames(as.matrix(gram4))
tetragramfreqs <- rowSums(as.matrix(gram4))
tetragramdata <- data.frame(term = tetragramterms, Frequency = tetragramfreqs)
tetragramdata <- tetragramdata[order(tetragramdata$Frequency,decreasing=TRUE),]
tetragramdata <- tetragramdata[tetragramdata$Frequency > 2,]
tetragramdata <- tetragramdata[!grepl(profanityregex,tetragramdata$term),]
rownames(tetragramdata) <- NULL
gram4 <- TermDocumentMatrix(CORPUS, control = list(tokenize = tetragram))
tetragramdata <- tetragramdata[tetragramdata$Frequency > 3,]
saveRDS(tetragramdata,'tetragramdata2.RDS')
tetragramdata <- readRDS('tetragramdata2.RDS')
tetragramdata <- readRDS('tetragramdata.RDS')
tetragramdata$term <- as.character(tetragramdata$term)
tetragramdata$term <- as.character(tetragramdata$term)
saveRDS(tetragramdata,'tetragramdata.RDS')
trigramdata <- readRDS('trigramdata.RDS')
trigramdata$term <- as.character(trigramdata$term)
saveRDS(trigramdata,'trigramdata.RDS')
bigramdata <- readRDS('bigramdata.RDS')
bigramdata$term <- as.character(bigramdata$term)
saveRDS(bigramdata,'bigramdata.RDS')
-1:3
rep(1,2)
?sample
# declare variables
probs <- c(0.01, 0.3, 0.58, 0.1, 0.01)
result <- -1:3
outcomes <- numeric()
# create outcome space
# for each result with an associated probability, create 100 * Probability copies for it
for(i in 1:length(probs)) {
outcomes <- c(outcomes, rep(result[i],probs[i]*100))
}
# given our outcome space, sample for it for our nextNum
nextNum <- function() {
sample(outcomes,1)
}
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
nextNum()
outcomes
length(outcomes)
probs[i]
probs[3]
probs[3] * 100
# declare variables
probs <- c(1, 30, 58, 10, 1)
result <- -1:3
outcomes <- numeric()
# create outcome space
# for each result with an associated probability, create 100 * Probability copies for it
for(i in 1:length(probs)) {
outcomes <- c(outcomes, rep(result[i],probs[i]))
}
# given our outcome space, sample for it for our nextNum
nextNum <- function() {
sample(outcomes,1)
}
outcomes
length(outcomes)
getwd()
setwd('C:/Users/user/Documents/GitHub/resume/')
setwd('C:/Users/user/Documents/GitHub/resume/')
